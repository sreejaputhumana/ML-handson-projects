{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-05T23:53:17.572088Z","iopub.status.idle":"2021-08-05T23:53:17.572653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Objective\nThe aim is to develop a model for predicting whether a tweet is about a real disaster or not.","metadata":{}},{"cell_type":"markdown","source":"# Rationale\n\nInspired from the below mentioned research article:\n\nAryan Karnati, Shashank Reddy Boyapally, D. S. K. . Natural Language Processing with Disaster tweets using Bidirectional LSTM. J. XI’AN Univ. Archit. Technol. 13, 1–6 (2021).","metadata":{}},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import  train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import  pad_sequences\nfrom tensorflow.keras.models import Model,Sequential\nfrom nltk.stem import PorterStemmer\nfrom keras.layers.embeddings import Embedding\nfrom keras import Input\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Flatten, Bidirectional,BatchNormalization\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.models import Model\nfrom keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T00:39:07.800343Z","iopub.execute_input":"2021-08-06T00:39:07.800708Z","iopub.status.idle":"2021-08-06T00:39:07.807874Z","shell.execute_reply.started":"2021-08-06T00:39:07.800683Z","shell.execute_reply":"2021-08-06T00:39:07.807047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"# load train data\ntrain_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:49.791764Z","iopub.execute_input":"2021-08-05T23:49:49.792106Z","iopub.status.idle":"2021-08-05T23:49:49.883971Z","shell.execute_reply.started":"2021-08-05T23:49:49.792078Z","shell.execute_reply":"2021-08-05T23:49:49.883035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load test data\ntest_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\") \ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:49.885343Z","iopub.execute_input":"2021-08-05T23:49:49.885623Z","iopub.status.idle":"2021-08-05T23:49:49.92134Z","shell.execute_reply.started":"2021-08-05T23:49:49.885596Z","shell.execute_reply":"2021-08-05T23:49:49.920369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:49.922489Z","iopub.execute_input":"2021-08-05T23:49:49.922746Z","iopub.status.idle":"2021-08-05T23:49:49.931097Z","shell.execute_reply.started":"2021-08-05T23:49:49.92272Z","shell.execute_reply":"2021-08-05T23:49:49.929511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data cleansing","metadata":{}},{"cell_type":"code","source":"# Checking if any duplicate records are present in both test and train data\ntest_duplicate = test_data[test_data.duplicated()] \ntest_duplicate\n\ntrain_duplicate = train_data[train_data.duplicated()] \ntrain_duplicate","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:49.932408Z","iopub.execute_input":"2021-08-05T23:49:49.932876Z","iopub.status.idle":"2021-08-05T23:49:49.968567Z","shell.execute_reply.started":"2021-08-05T23:49:49.932838Z","shell.execute_reply":"2021-08-05T23:49:49.967459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We found no duplicates in both test and train datasets, hence we proceed for further cleansing of data.","metadata":{}},{"cell_type":"code","source":"#Remove unwanted colums\ntest_data = test_data.drop('location',axis = 1) \ntrain_data = train_data.drop('location',axis = 1) ","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:49.971596Z","iopub.execute_input":"2021-08-05T23:49:49.971882Z","iopub.status.idle":"2021-08-05T23:49:49.97816Z","shell.execute_reply.started":"2021-08-05T23:49:49.971854Z","shell.execute_reply":"2021-08-05T23:49:49.977279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleanhtml(raw_html):\n  cleanr = re.compile('<.*?>')\n  cleantext = re.sub(cleanr, '', raw_html)\n  return cleantext","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:49.979893Z","iopub.execute_input":"2021-08-05T23:49:49.980142Z","iopub.status.idle":"2021-08-05T23:49:49.989301Z","shell.execute_reply.started":"2021-08-05T23:49:49.980119Z","shell.execute_reply":"2021-08-05T23:49:49.988803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove URL and html tags from the text\ntrain_data['clean_text'] = train_data['text'].apply(cleanhtml)\n\ntest_data['clean_text'] = test_data['text'].apply(cleanhtml)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:49.989982Z","iopub.execute_input":"2021-08-05T23:49:49.990165Z","iopub.status.idle":"2021-08-05T23:49:50.028038Z","shell.execute_reply.started":"2021-08-05T23:49:49.990144Z","shell.execute_reply":"2021-08-05T23:49:50.026886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleansing_data(text):\n    cln_text = re.sub('[^a-zA-Z \\n\\.]', '', text)\n    cln_text = cln_text.lower()\n    cln_text = cln_text.split()\n    cln_text = ' '.join(cln_text)   \n    return cln_text","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:50.029793Z","iopub.execute_input":"2021-08-05T23:49:50.03019Z","iopub.status.idle":"2021-08-05T23:49:50.035748Z","shell.execute_reply.started":"2021-08-05T23:49:50.030161Z","shell.execute_reply":"2021-08-05T23:49:50.034509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove speicial characters\ntest_data['clean_text'] = test_data['clean_text'].apply(cleansing_data)\ntrain_data['clean_text'] = train_data['clean_text'].apply(cleansing_data)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:50.037332Z","iopub.execute_input":"2021-08-05T23:49:50.037721Z","iopub.status.idle":"2021-08-05T23:49:50.136813Z","shell.execute_reply.started":"2021-08-05T23:49:50.037682Z","shell.execute_reply":"2021-08-05T23:49:50.135462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filter_text(text):\n    text = text.split()\n    text = [ps.stem(word) for word in text if word not in stopwords.words('english')]\n    text = ' '.join(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:50.138471Z","iopub.execute_input":"2021-08-05T23:49:50.138857Z","iopub.status.idle":"2021-08-05T23:49:50.145376Z","shell.execute_reply.started":"2021-08-05T23:49:50.138817Z","shell.execute_reply":"2021-08-05T23:49:50.144133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove suffixes and prefixes\nps = PorterStemmer()\n\ntrain_data['filtered_text'] = train_data['clean_text'].apply(filter_text)\ntest_data['filtered_text'] =  test_data['clean_text'].apply(filter_text)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:49:50.146835Z","iopub.execute_input":"2021-08-05T23:49:50.147205Z","iopub.status.idle":"2021-08-05T23:50:19.519185Z","shell.execute_reply.started":"2021-08-05T23:49:50.147166Z","shell.execute_reply":"2021-08-05T23:50:19.518307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train_data['filtered_text']\ny_train = train_data['target']\nx_test = test_data['filtered_text']","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:50:19.526691Z","iopub.execute_input":"2021-08-05T23:50:19.527038Z","iopub.status.idle":"2021-08-05T23:50:19.543648Z","shell.execute_reply.started":"2021-08-05T23:50:19.527001Z","shell.execute_reply":"2021-08-05T23:50:19.542243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_size = 20000\nsent_len = 100\nembed_vector_len = 100\ntokenizer = Tokenizer(num_words = max_size)\ntokenizer.fit_on_texts(x_train)\nwords_to_index = tokenizer.word_index\nseq_xtrain = tokenizer.texts_to_sequences(x_train)\nseq_xtest = tokenizer.texts_to_sequences(x_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:50:19.544946Z","iopub.execute_input":"2021-08-05T23:50:19.545229Z","iopub.status.idle":"2021-08-05T23:50:20.267796Z","shell.execute_reply.started":"2021-08-05T23:50:19.545194Z","shell.execute_reply":"2021-08-05T23:50:20.266666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One-Hot encoding on datasets","metadata":{}},{"cell_type":"code","source":"onehot_train = [one_hot(words,max_size)for words in x_train] \nonehot_test = [one_hot(words,max_size)for words in x_test] ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding representation \n\nWord Embeddings are vector representations of words that help us to retrieve linear substructures at the same time  process the text inorder for the model to understand. Usually, word embeddings are weights of the hidden layer of the neural network architecture, after the defined model converges on the cost function.","metadata":{}},{"cell_type":"code","source":"#embed\nembedded_train = pad_sequences(onehot_train, padding='pre', maxlen=sent_len)\nprint(embedded_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:50:20.269046Z","iopub.execute_input":"2021-08-05T23:50:20.269326Z","iopub.status.idle":"2021-08-05T23:50:20.337789Z","shell.execute_reply.started":"2021-08-05T23:50:20.269296Z","shell.execute_reply":"2021-08-05T23:50:20.336539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedded_test = pad_sequences(onehot_test, padding='pre', maxlen=sent_len)\nprint(embedded_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:50:20.338906Z","iopub.execute_input":"2021-08-05T23:50:20.339168Z","iopub.status.idle":"2021-08-05T23:50:20.373678Z","shell.execute_reply.started":"2021-08-05T23:50:20.339141Z","shell.execute_reply":"2021-08-05T23:50:20.372666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array(embedded_train)\n\ny = np.array( train_data['target'])\n\nx_test = np.array(embedded_test)\n\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.20,\n                                                    random_state=45, shuffle=True)\n\nx_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-06T00:30:59.562112Z","iopub.execute_input":"2021-08-06T00:30:59.562471Z","iopub.status.idle":"2021-08-06T00:30:59.573747Z","shell.execute_reply.started":"2021-08-06T00:30:59.562441Z","shell.execute_reply":"2021-08-06T00:30:59.572833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Global Vectors(GloVE**) algorithm is used to obtain the vector epresentation of words.\n**Word2Vec** relates the target words to their respective context without carrying any additional informations.\nIn comparisoon with Word2Vec, GloVe builds word embeddings in such a way that a combination of word vectors relates directly to the probability of those words’ co-occurrence in the corpus.","metadata":{}},{"cell_type":"code","source":"def read_glove_vector(glove_vec):\n  with open(glove_vec, 'r', encoding='UTF-8') as f:\n    words = set()\n    word_to_vec_map = {}\n    for line in f:\n      w_line = line.split()\n      curr_word = w_line[0]\n      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n\n\n\n  return word_to_vec_map","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:50:20.392893Z","iopub.execute_input":"2021-08-05T23:50:20.393145Z","iopub.status.idle":"2021-08-05T23:50:20.399211Z","shell.execute_reply.started":"2021-08-05T23:50:20.393122Z","shell.execute_reply":"2021-08-05T23:50:20.398191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_vec_map = read_glove_vector('../input/glove6b100dtxt/glove.6B.100d.txt')\n\n\n\ntweet_max_length = max([len(i) for i in train_data['text']])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-05T23:50:20.400377Z","iopub.execute_input":"2021-08-05T23:50:20.400689Z","iopub.status.idle":"2021-08-05T23:50:36.359341Z","shell.execute_reply.started":"2021-08-05T23:50:20.400653Z","shell.execute_reply":"2021-08-05T23:50:36.358438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model creation","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_size, embed_vector_len, input_length=sent_len))\nmodel.add(Bidirectional(LSTM(30, dropout=0.2)))\nmodel.add(BatchNormalization())\n#model.add(Dense(sent_len, activation = \"relu\"))\n#model.add(Dropout(0.5))\nmodel.add(Dense(20, activation = \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\n\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-08-06T00:34:56.147727Z","iopub.execute_input":"2021-08-06T00:34:56.148118Z","iopub.status.idle":"2021-08-06T00:34:56.688084Z","shell.execute_reply.started":"2021-08-06T00:34:56.148093Z","shell.execute_reply":"2021-08-06T00:34:56.686504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes=True, to_file='model.png')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T00:35:01.32565Z","iopub.execute_input":"2021-08-06T00:35:01.325978Z","iopub.status.idle":"2021-08-06T00:35:01.47577Z","shell.execute_reply.started":"2021-08-06T00:35:01.325943Z","shell.execute_reply":"2021-08-06T00:35:01.474272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"#mod_train = model.fit(train_data,y_train,epochs=5,validation_data=(test_data,y_test))\nmodel_train = model.fit(x_train,y_train,validation_data=(x_valid,y_valid),epochs=10,batch_size=64)\n#X_train.info","metadata":{"execution":{"iopub.status.busy":"2021-08-06T00:35:12.099822Z","iopub.execute_input":"2021-08-06T00:35:12.100209Z","iopub.status.idle":"2021-08-06T00:36:33.217672Z","shell.execute_reply.started":"2021-08-06T00:35:12.100176Z","shell.execute_reply":"2021-08-06T00:36:33.216847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model prediction and evaluation","metadata":{}},{"cell_type":"code","source":"y_valid_pred = model.predict_classes(x_valid)\n\nprint(confusion_matrix(y_valid,y_valid_pred))\nprint('Accuracy Score :',accuracy_score(y_valid, y_valid_pred))\nprint('Report : ')\nprint(classification_report(y_valid, y_valid_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T00:39:57.703932Z","iopub.execute_input":"2021-08-06T00:39:57.704326Z","iopub.status.idle":"2021-08-06T00:39:58.313218Z","shell.execute_reply.started":"2021-08-06T00:39:57.704297Z","shell.execute_reply":"2021-08-06T00:39:58.311924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting values\ny_pred = model.predict_classes(x_test).flatten()\n\n\nsubmission = pd.DataFrame({\n       \"id\": test_data['id'],\n      \"target\": y_pred\n    })\nsubmission.to_csv(r\"C:\\Users\\sreej\\Documents\\Kaggle\\NLP Tweet classification\\submission_tweet.csv\", index=False)\nsubmission.head(12)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T01:14:26.166943Z","iopub.execute_input":"2021-08-06T01:14:26.167292Z","iopub.status.idle":"2021-08-06T01:14:27.412944Z","shell.execute_reply.started":"2021-08-06T01:14:26.167261Z","shell.execute_reply":"2021-08-06T01:14:27.412021Z"},"trusted":true},"execution_count":null,"outputs":[]}]}