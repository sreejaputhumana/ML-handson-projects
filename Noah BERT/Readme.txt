                      Instruction for Execution
		      --------------------------

Introduction:
         In this study, we investigate the efficacy of integrating attention mechanisms into Variational Autoencoders (VAEs) for enhancing generative model performance. Leveraging ResNet blocks and custom attention layers, we demonstrate significant improvements in model accuracy, as quantified by the Evidence Lower Bound (ELBO) and Frechet Inception Distance (FID) metrics. Our findings underscore the potential of attention-based architectures in refining latent space representations and reconstruction quality in deep generative models.

Platform and version:
	  python 3.10.9 
          tensor flow 2.13.0 

Required Python Libraries :
	numpy
	tensorflow
        tensorflow-addons
	IPython
	matplotlib
	scikit-learn


	  




